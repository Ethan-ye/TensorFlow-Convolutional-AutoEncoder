{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T13:29:56.432190Z",
     "start_time": "2019-12-05T13:29:52.544583Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import graph_util\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "from models import *\n",
    "from mnist import MNIST  # this is the MNIST data manager that provides training/testing batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T13:29:57.740712Z",
     "start_time": "2019-12-05T13:29:57.685846Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder(object):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        build the graph\n",
    "        \"\"\"\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # place holder of input data\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])  # [#batch, img_height, img_width, #channels]\n",
    "#         code = tf.placeholder(tf.float32, shape=[None, 20])  # [#batch, features]\n",
    "        # encode\n",
    "        conv1 = Convolution2D([3, 3, 1, 32], activation=tf.nn.relu, scope='conv_1')(x)\n",
    "        pool1 = MaxPooling(kernel_shape=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', scope='pool_1')(conv1)\n",
    "        conv2 = Convolution2D([3, 3, 32, 32], activation=tf.nn.relu, scope='conv_2')(pool1)\n",
    "        pool2 = MaxPooling(kernel_shape=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', scope='pool_2')(conv2)\n",
    "        #卷据池化 pass 10000, training loss 778.7437744140625 trained:1000000, 2511.816041 sec\n",
    "#         conv3 = Convolution2D([3, 3, 32, 32], activation=tf.nn.relu, scope='conv_3')(pool2)\n",
    "#         pool3 = MaxPooling(kernel_shape=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', scope='pool_3')(conv3)\n",
    "        #只卷积 pass 10000, training loss 586.9559326171875  trained:1000000, 1589.608651 sec\n",
    "        conv3 = Convolution2D([3, 3, 32, 32], strides=[1, 2, 2, 1], activation=tf.nn.relu, scope='conv_3')(pool2)\n",
    "        pool3 = conv3\n",
    "        \n",
    "        unfold = Unfold(scope='unfold')(pool3)\n",
    "#         fc4 = FullyConnected(128, activation=tf.nn.relu, scope='fc4')(unfold)\n",
    "        fc4 = unfold\n",
    "        encoded = FullyConnected(20, activation=tf.nn.relu, scope='encode')(fc4)\n",
    "#         pass 10000, training loss 1393.703857421875 # trained:1000000, 5059.127015 sec 5 feature\n",
    "#         pass 10000, training loss 1324.6783447265625 trained:1000000, 1733.352743 sec 5feature \n",
    "    # pass 10000, training loss 993.19921875 trained:1000000, 4314.145067 sec 10\n",
    "#     pass 10000, training loss 676.6449584960938 # trained:1000000, 1598.109703 20\n",
    "        # decode\n",
    "#         dfc4 = FullyConnected(128, activation=tf.nn.relu, scope='dfc4')(encoded)\n",
    "        dfc4 = encoded\n",
    "        decoded = FullyConnected(4*4*32, activation=tf.nn.relu, scope='decode')(dfc4)\n",
    "        fold = Fold([-1, 4, 4, 32], scope='fold')(decoded)\n",
    "        \n",
    "        unpool3 = fold\n",
    "        deconv3 = DeConvolution2D([3, 3, 32, 32], strides=[1, 2, 2, 1], output_shape=tf.shape(pool2), activation=tf.nn.relu, scope='deconv_3')(unpool3)\n",
    "#         unpool3 = UnPooling((2, 2), output_shape=tf.shape(conv3), scope='unpool_3')(fold)\n",
    "#         deconv3 = DeConvolution2D([3, 3, 32, 32], output_shape=tf.shape(pool2), activation=tf.nn.relu, scope='deconv_3')(unpool3)\n",
    "        unpool2 = UnPooling((2, 2), output_shape=tf.shape(conv2), scope='unpool_2')(deconv3)\n",
    "        deconv2 = DeConvolution2D([3, 3, 32, 32], output_shape=tf.shape(pool1), activation=tf.nn.relu, scope='deconv_2')(unpool2)\n",
    "        unpool1 = UnPooling((2, 2), output_shape=tf.shape(conv1), scope='unpool_2')(deconv2)\n",
    "        reconstruction = DeConvolution2D([3, 3, 1, 32], output_shape=tf.shape(x), activation=tf.nn.sigmoid, scope='deconv_1')(unpool1)\n",
    "        # loss function\n",
    "        loss = tf.nn.l2_loss(x - reconstruction)  # L2 loss\n",
    "\n",
    "        # training\n",
    "        training = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "        #\n",
    "        self.x = x\n",
    "        self.reconstruction = reconstruction\n",
    "        self.loss = loss\n",
    "        self.training = training\n",
    "        self.encoded = encoded\n",
    "#         self.dfc4 = dfc4\n",
    "\n",
    "    def train(self, batch_size, passes, new_training=True):\n",
    "        \"\"\"\n",
    "\n",
    "        :param batch_size:\n",
    "        :param passes:\n",
    "        :param new_training:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        mnist = MNIST()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # prepare session\n",
    "            if new_training:\n",
    "                saver, global_step = Model.start_new_session(sess)\n",
    "            else:\n",
    "                saver, global_step = Model.continue_previous_session(sess, ckpt_file='saver/checkpoint')\n",
    "\n",
    "            # start training, 需要修改数据获取的方法\n",
    "            start = time.time()\n",
    "            for step in range(1+global_step, 1+passes+global_step):\n",
    "                x, y = mnist.get_batch(batch_size)\n",
    "                self.training.run(feed_dict={self.x: x})\n",
    "\n",
    "                if step % 10 == 0:\n",
    "                    loss = self.loss.eval(feed_dict={self.x: x})\n",
    "                    print(\"pass {}, training loss {}\".format(step, loss))\n",
    "\n",
    "                if step % 100 == 0:  \n",
    "                    print('trained:%d, %f sec' % ((step-global_step)*batch_size, time.time() - start))\n",
    "                \n",
    "                if step % 1000 == 0:  # save weights\n",
    "                    saver.save(sess, 'saver/cnn', global_step=step)\n",
    "                    print('checkpoint saved')\n",
    "\n",
    "    def weights_to_grid(self, weights, rows, cols):\n",
    "        \"\"\"convert the weights tensor into a grid for visualization\"\"\"\n",
    "        height, width, in_channel, out_channel = weights.shape\n",
    "        padded = np.pad(weights, [(1, 1), (1, 1), (0, 0), (0, rows * cols - out_channel)],\n",
    "                            mode='constant', constant_values=0)\n",
    "        transposed = padded.transpose((3, 1, 0, 2))\n",
    "        reshaped = transposed.reshape((rows, -1))\n",
    "        grid_rows = [row.reshape((-1, height + 2, in_channel)).transpose((1, 0, 2)) for row in reshaped]\n",
    "        grid = np.concatenate(grid_rows, axis=0)\n",
    "        return grid.squeeze()\n",
    "                    \n",
    "    def reconstruct(self):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        mnist = MNIST()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver, global_step = Model.continue_previous_session(sess, ckpt_file='saver/checkpoint')\n",
    "\n",
    "            # visualize weights\n",
    "            first_layer_weights = tf.get_default_graph().get_tensor_by_name(\"conv_1/kernel:0\").eval()\n",
    "            grid_image = self.weights_to_grid(first_layer_weights, 4, 8)\n",
    "\n",
    "            fig, ax0 = plt.subplots(ncols=1, figsize=(8, 4))\n",
    "            ax0.imshow(grid_image, cmap=plt.cm.gray, interpolation='nearest')\n",
    "            ax0.set_title('first conv layers weights')\n",
    "            plt.show()\n",
    "\n",
    "            # visualize results\n",
    "            batch_size = 36\n",
    "            x, y = mnist.get_batch(batch_size, dataset='testing')\n",
    "            org, recon = sess.run((self.x, self.reconstruction), feed_dict={self.x: x})\n",
    "\n",
    "            input_images = self.weights_to_grid(org.transpose((1, 2, 3, 0)), 6, 6)\n",
    "            recon_images = self.weights_to_grid(recon.transpose((1, 2, 3, 0)), 6, 6)\n",
    "\n",
    "            fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "            ax0.imshow(input_images, cmap=plt.cm.gray, interpolation='nearest')\n",
    "            ax0.set_title('input images')\n",
    "            ax1.imshow(recon_images, cmap=plt.cm.gray, interpolation='nearest')\n",
    "            ax1.set_title('reconstructed images')\n",
    "            plt.show()\n",
    "            \n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "#         def weights_to_grid(weights, rows, cols):\n",
    "#             \"\"\"convert the weights tensor into a grid for visualization\"\"\"\n",
    "#             height, width, in_channel, out_channel = weights.shape\n",
    "#             padded = np.pad(weights, [(1, 1), (1, 1), (0, 0), (0, rows * cols - out_channel)],\n",
    "#                             mode='constant', constant_values=0)\n",
    "#             transposed = padded.transpose((3, 1, 0, 2))\n",
    "#             reshaped = transposed.reshape((rows, -1))\n",
    "#             grid_rows = [row.reshape((-1, height + 2, in_channel)).transpose((1, 0, 2)) for row in reshaped]\n",
    "#             grid = np.concatenate(grid_rows, axis=0)\n",
    "\n",
    "#             return grid.squeeze()\n",
    "\n",
    "        mnist = MNIST()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver, global_step = Model.continue_previous_session(sess, ckpt_file='saver/checkpoint')\n",
    "\n",
    "            codes = np.empty([50000,20], dtype = np.float32)\n",
    "            labels = np.empty([50000,1], dtype = np.int)\n",
    "            batch_size = 100\n",
    "            start = time.time()\n",
    "            for i in range(500):\n",
    "                x, y = mnist.get_batch(batch_size, dataset='training')\n",
    "                codes[i*batch_size:(i+1)*batch_size,] = sess.run((self.encoded), feed_dict={self.x: x}) \n",
    "                labels[i*batch_size:(i+1)*batch_size,] = y@(np.arange(10).reshape(10,1))\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print('encoded:%d, %f sec' % ((i+1)*batch_size, time.time() - start))\n",
    "                \n",
    "            return codes,labels\n",
    "#             batch_size = 10000\n",
    "#             x, y = mnist.get_batch(batch_size, dataset='testing')\n",
    "#             org, recon = sess.run((self.x, self.reconstruction), feed_dict={self.x: x})\n",
    "\n",
    "#             input_images = weights_to_grid(org.transpose((1, 2, 3, 0)), 6, 6)\n",
    "#             recon_images = weights_to_grid(recon.transpose((1, 2, 3, 0)), 6, 6)\n",
    "\n",
    "#             fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "#             ax0.imshow(input_images, cmap=plt.cm.gray, interpolation='nearest')\n",
    "#             ax0.set_title('input images')\n",
    "#             ax1.imshow(recon_images, cmap=plt.cm.gray, interpolation='nearest')\n",
    "#             ax1.set_title('reconstructed images')\n",
    "#             plt.show()\n",
    "\n",
    "    def decode(self, codes):\n",
    "        with tf.Session() as sess:\n",
    "            saver, global_step = Model.continue_previous_session(sess, ckpt_file='saver/checkpoint')\n",
    "            output_graph_def = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['deconv_1/Sigmoid'])\n",
    "\n",
    "        input_node_names_list = ['encode/Relu']\n",
    "        inputs_replaced_graph_def = tf.GraphDef()\n",
    "        for node in output_graph_def.node:\n",
    "            if node.name in input_node_names_list:\n",
    "                placeholder_node = tf.NodeDef()\n",
    "                placeholder_node.op = \"Placeholder\"\n",
    "                placeholder_node.name = node.name\n",
    "                placeholder_node.attr[\"dtype\"].CopyFrom(\n",
    "                    tf.AttrValue(type=tf.float32.as_datatype_enum))\n",
    "                inputs_replaced_graph_def.node.extend([placeholder_node])\n",
    "            else:\n",
    "                other_node = tf.NodeDef()\n",
    "                other_node.CopyFrom(node)\n",
    "                inputs_replaced_graph_def.node.extend([other_node])\n",
    "\n",
    "        decode_graph_def = graph_util.extract_sub_graph(inputs_replaced_graph_def, ['deconv_1/Sigmoid'])\n",
    "        decode_graph = tf.Graph()\n",
    "        \n",
    "        with decode_graph.as_default():\n",
    "            input_x,input_y,result = tf.import_graph_def(decode_graph_def, return_elements=[\"encode/Relu:0\",\"Placeholder:0\",\"deconv_1/Sigmoid:0\"])\n",
    "\n",
    "            n = codes.shape[0]\n",
    "            if n>100: n=100 \n",
    "            fills = np.empty([n,28,28,1], dtype=np.float32)\n",
    "                \n",
    "            with tf.Session() as sess:\n",
    "                img = sess.run(result,feed_dict={input_x:codes[0:n,:],input_y:fills})\n",
    "                    \n",
    "        num_grids = int(np.ceil(np.sqrt(n)))\n",
    "        images = self.weights_to_grid(img.transpose((1, 2, 3, 0)), num_grids, num_grids)\n",
    "\n",
    "        fig, ax = plt.subplots(ncols=1, figsize=(10, 5))\n",
    "        ax.imshow(images, cmap=plt.cm.gray, interpolation='nearest')\n",
    "        plt.show()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T13:30:05.585726Z",
     "start_time": "2019-12-05T13:30:04.854681Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_autoencoder = ConvolutionalAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T13:30:21.260837Z",
     "start_time": "2019-12-05T13:30:21.256843Z"
    }
   },
   "outputs": [],
   "source": [
    "# conv_autoencoder.train(batch_size=100, passes=10000, new_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T13:30:24.347581Z",
     "start_time": "2019-12-05T13:30:23.134825Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_autoencoder.reconstruct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T13:30:53.965423Z",
     "start_time": "2019-12-05T13:30:31.924334Z"
    }
   },
   "outputs": [],
   "source": [
    "codes,labels = conv_autoencoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T13:31:01.609992Z",
     "start_time": "2019-12-05T13:31:00.974689Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_autoencoder.decode(codes[0:36,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-05T11:56:57.373391Z",
     "start_time": "2019-12-05T11:56:57.368401Z"
    }
   },
   "outputs": [],
   "source": [
    "# code_test = np.array([[10.      ,  0,  0 ,  0 ,  0,\n",
    "#           0.      ,  0 ,  0 ,0 ,  0 ,\n",
    "#          0 ,  0 , 0,  0,  0 ,\n",
    "#         0 , 0 ,   0.      ,  0,   0.]],dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:02:24.016540Z",
     "start_time": "2019-12-03T15:02:24.011553Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "PCA_2D = PCA(n_components=2)#2主成分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:02:25.786087Z",
     "start_time": "2019-12-03T15:02:25.696365Z"
    }
   },
   "outputs": [],
   "source": [
    "minist_2D = PCA_2D.fit_transform(codes)#投影到2主成分空间\n",
    "minist_2D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:04:56.221787Z",
     "start_time": "2019-12-03T15:04:53.610145Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "cax = ax.scatter(minist_2D[0:50000,0], minist_2D[0:50000,1], c=labels[0:50000,0], alpha=0.5)# \n",
    "fig.colorbar(cax)\n",
    "ax.set_xlabel(r'$\\Delta_i$', fontsize=15)\n",
    "ax.set_ylabel(r'$\\Delta_{i+1}$', fontsize=15)\n",
    "ax.set_title('Volume and percent change')\n",
    "\n",
    "ax.grid(True)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:52:40.745660Z",
     "start_time": "2019-12-03T15:12:24.515295Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "start = time.time()\n",
    "model = TSNE(learning_rate=100, n_components=2, random_state=0, perplexity=5)\n",
    "tsne5 = model.fit_transform(codes)\n",
    "print('tsne5 finish，{}'.format(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "model = TSNE(learning_rate=100, n_components=2, random_state=0, perplexity=30)\n",
    "tsne30 = model.fit_transform(codes)\n",
    "print('tsne30 finish，{}'.format(time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "model = TSNE(learning_rate=100, n_components=2, random_state=0, perplexity=50)\n",
    "tsne50 = model.fit_transform(codes)\n",
    "print('tsne50 finish，{}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T23:55:37.411267Z",
     "start_time": "2019-12-03T23:55:33.568501Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(24, 48))\n",
    "plt.subplot(311)\n",
    "plt.scatter(tsne5[:, 0], tsne5[:, 1], c=labels[:,0], alpha=0.5)\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.scatter(tsne30[:, 0], tsne30[:, 1], c=labels[:,0], alpha=0.5)\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.scatter(tsne50[:, 0], tsne50[:, 1], c=labels[:,0], alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
